{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction and Libraries Setup\n\nIn this notebook, I'm working on **Kaggle’s \"GANs Getting Started\"** competition using the **Monet Painting Dataset (New)**. The main goal here is to **convert regular photos into paintings** that **look like Monet's artwork**. I'm using a **special type of generative model** called **CycleGAN**, because it can **learn how to translate between two image styles** without needing **paired images**. This makes it **very useful for this task**.\n\nFirst, I'll start by **importing all the libraries I need**. I'm using **TensorFlow, NumPy, and Matplotlib**, because they **help me handle images easily, build neural networks, and visualize results clearly**.\n\nAfter I **import these libraries**, I'll **set up the paths to my datasets**. For this competition I have:\n- **Monet paintings** (the new dataset) with 300 JPG images, each 256 pixels by 256 pixels with 3 color channels  \n- **Regular photos** with 7,038 JPG images, each 256 pixels by 256 pixels with 3 color channels  \n\nAll of these images will be **normalized** so that their pixel values are scaled to lie between –1 and 1 before I train my model.\n\nThen I'll **verify** that these **folders are available** on my system and **count the images**, just to make sure everything is ready to go.\n","metadata":{}},{"cell_type":"code","source":"import os, glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport random\n\nprint(\"TensorFlow\", tf.__version__)\n\n# Kaggle input directory for this competition\nBASE_PATH = \"/kaggle/input/gan-getting-started\"\nMONET_DIR = os.path.join(BASE_PATH, \"monet_jpg\")\nPHOTO_DIR = os.path.join(BASE_PATH, \"photo_jpg\")\n\n# Ensure paths exist\nassert os.path.isdir(MONET_DIR), f\"Monet directory not found at {MONET_DIR}\"\nassert os.path.isdir(PHOTO_DIR), f\"Photo directory not found at {PHOTO_DIR}\"\n\n# Gather and print image counts\nmonet_paths = glob.glob(os.path.join(MONET_DIR, \"*.jpg\"))\nphoto_paths = glob.glob(os.path.join(PHOTO_DIR, \"*.jpg\"))\nprint(f\"Found {len(monet_paths)} Monet paintings\")\nprint(f\"Found {len(photo_paths)} photos\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:58:50.900102Z","iopub.execute_input":"2025-05-25T12:58:50.900764Z","iopub.status.idle":"2025-05-25T12:59:04.767867Z","shell.execute_reply.started":"2025-05-25T12:58:50.900736Z","shell.execute_reply":"2025-05-25T12:59:04.767006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Visualizing Sample Images\n\nNow I **want to quickly look at a few images** from both datasets. \n\nSo, I **created a simple function** called `show` to **randomly pick and display samples** of **Monet paintings and normal photos side by side**. \n\nThis step **helps me visually understand** how the two datasets **differ from each other**, which is **really important before training my GAN model**.\n","metadata":{}},{"cell_type":"code","source":"def show(paths,title,n=4):\n    plt.figure(figsize=(12,3));\n    for i,p in enumerate(random.sample(paths,n)):\n        plt.subplot(1,n,i+1); plt.imshow(Image.open(p)); plt.axis('off')\n    plt.suptitle(title); plt.show()\nshow(monet_paths,'Monet samples'); show(photo_paths,'Photo samples')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:59:04.769411Z","iopub.execute_input":"2025-05-25T12:59:04.769899Z","iopub.status.idle":"2025-05-25T12:59:05.299506Z","shell.execute_reply.started":"2025-05-25T12:59:04.769864Z","shell.execute_reply":"2025-05-25T12:59:05.298253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. RGB Channel Histograms for Monet vs Photo\n\nI **wanted to understand how colors differ** between **Monet paintings and the regular photos**, so I **wrote a function** to **plot RGB color histograms** for both types of images. \n\nBy **looking at these histograms**, I can **clearly see if there are noticeable differences** in the **color distribution**, which could **influence how my CycleGAN learns to translate images** from one style to another.\n","metadata":{}},{"cell_type":"code","source":"\n# RGB channel histograms for Monet vs Photo\ndef rgb_histograms(paths, n=200):\n    arr = []\n    for p in random.sample(paths, min(n,len(paths))):\n        img = np.array(Image.open(p).resize((256,256))) / 255.0\n        arr.append(img.reshape(-1,3))\n    arr = np.concatenate(arr, axis=0)\n    return arr\nmon_rgb = rgb_histograms(monet_paths)\npho_rgb = rgb_histograms(photo_paths)\n\nplt.figure(figsize=(12,4))\nfor i,col in enumerate(['R','G','B']):\n    plt.subplot(1,3,i+1)\n    plt.hist(mon_rgb[:,i], bins=50, alpha=0.6, label='Monet')\n    plt.hist(pho_rgb[:,i], bins=50, alpha=0.6, label='Photo')\n    plt.title(f'{col}-channel')\n    plt.legend()\nplt.suptitle('Colour Distribution Comparison'); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:59:05.300764Z","iopub.execute_input":"2025-05-25T12:59:05.301139Z","iopub.status.idle":"2025-05-25T12:59:12.133241Z","shell.execute_reply.started":"2025-05-25T12:59:05.301092Z","shell.execute_reply":"2025-05-25T12:59:12.132534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Preparing the Data Pipeline\n\nBefore **training the GAN**, I **needed a reliable way** to **load and preprocess the images**. \n\nSo, I **wrote the `decode` function** to **convert image files into normalized tensors**. \n\nI also **created a dataset pipeline** using **TensorFlow’s Dataset API**, because it's **efficient at handling image data**. \n\nThis pipeline **makes sure images are resized correctly** and **batches are ready for training smoothly**.\n","metadata":{}},{"cell_type":"code","source":"AUTOTUNE=tf.data.AUTOTUNE\ndef decode(p):\n    img=tf.io.read_file(p); img=tf.image.decode_jpeg(img,3); img=tf.image.convert_image_dtype(img,tf.float32);\n    img=(img*2.)-1.; return tf.image.resize(img,[256,256])\ndef dataset(paths,batch=1):\n    return (tf.data.Dataset.from_tensor_slices(paths)\n            .shuffle(len(paths)).map(decode,num_parallel_calls=AUTOTUNE)\n            .repeat().batch(batch).prefetch(AUTOTUNE))\nphoto_ds, monet_ds = dataset(photo_paths), dataset(monet_paths)\nBATCH=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:59:12.133893Z","iopub.execute_input":"2025-05-25T12:59:12.134113Z","iopub.status.idle":"2025-05-25T12:59:13.451291Z","shell.execute_reply.started":"2025-05-25T12:59:12.134096Z","shell.execute_reply":"2025-05-25T12:59:13.450477Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Implementing Instance Normalization\n\nFor my **CycleGAN model**, I **needed a custom normalization technique** called **Instance Normalization**, which **isn't directly available** in TensorFlow's built-in layers. \n\nSo, I **created my own custom class** named `InstanceNorm`. \n\nThis **special normalization** helps my model **capture styles and textures better**, which is **crucial when trying to produce images** that **look like genuine Monet paintings**.","metadata":{}},{"cell_type":"code","source":"class InstanceNorm(layers.Layer):\n    def __init__(self, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n\n    def build(self, shape):\n        self.g = self.add_weight(shape=(shape[-1],), initializer=tf.random_normal_initializer(1., 0.02), trainable=True)\n        self.b = self.add_weight(shape=(shape[-1],), initializer='zeros', trainable=True)\n\n    def call(self, x):\n        m, v = tf.nn.moments(x, [1, 2], keepdims=True)\n        return self.g * (x - m) / tf.sqrt(v + self.eps) + self.b\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:59:13.453178Z","iopub.execute_input":"2025-05-25T12:59:13.453392Z","iopub.status.idle":"2025-05-25T12:59:13.458470Z","shell.execute_reply.started":"2025-05-25T12:59:13.453374Z","shell.execute_reply":"2025-05-25T12:59:13.457735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Building the Generator and Discriminator Models\n\nNow, I **define the generator and discriminator models** for my **CycleGAN**. \n\nThe **generator** takes an image and **translates it into the other style**, in this case, **converting photos into Monet-style paintings**. \n\nI **designed the generator** with **convolutional and residual blocks** to **effectively capture details and textures**. \n\nSimilarly, the **discriminator** is used to **distinguish real images from generated ones**, which **guides my model towards generating more realistic paintings**. \n\nAfter **defining these models**, I **initialize them** so they're **ready for training**.\n","metadata":{}},{"cell_type":"code","source":"def gen():\n    inp = layers.Input((256, 256, 3))\n    x = layers.Conv2D(64, 7, padding='same')(inp)\n    x = InstanceNorm()(x)\n    x = layers.ReLU()(x)\n\n    for f in [128, 256]:\n        x = layers.Conv2D(f, 3, strides=2, padding='same')(x)\n        x = InstanceNorm()(x)\n        x = layers.ReLU()(x)\n\n    def res(x):\n        y = layers.Conv2D(256, 3, padding='same')(x)\n        y = InstanceNorm()(y)\n        y = layers.ReLU()(y)\n        \n        y = layers.Conv2D(256, 3, padding='same')(y)\n        y = InstanceNorm()(y)\n        return layers.add([x, y])\n\n    for _ in range(6):\n        x = res(x)\n\n    for f in [128, 64]:\n        x = layers.Conv2DTranspose(f, 3, strides=2, padding='same', output_padding=1)(x)\n        x = InstanceNorm()(x)\n        x = layers.ReLU()(x)\n\n    out = layers.Conv2D(3, 7, padding='same', activation='tanh')(x)\n    return tf.keras.Model(inp, out)\n\ndef disc():\n    inp = layers.Input((256, 256, 3))\n    x = layers.Conv2D(64, 4, strides=2, padding='same')(inp)\n    x = layers.LeakyReLU(0.2)(x)\n\n    for f in [128, 256, 512]:\n        s = 1 if f == 512 else 2\n        x = layers.Conv2D(f, 4, strides=s, padding='same')(x)\n        x = InstanceNorm()(x)\n        x = layers.LeakyReLU(0.2)(x)\n\n    out = layers.Conv2D(1, 4, padding='same')(x)\n    return tf.keras.Model(inp, out)\n\nG, F, DM, DP = gen(), gen(), disc(), disc()\nprint('Models ready')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:59:13.458983Z","iopub.execute_input":"2025-05-25T12:59:13.459189Z","iopub.status.idle":"2025-05-25T12:59:15.496804Z","shell.execute_reply.started":"2025-05-25T12:59:13.459173Z","shell.execute_reply":"2025-05-25T12:59:15.496188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Defining the Training Step\n\nNext, I **defined my training step function**, which is the **core part of training the CycleGAN**. \n\nInside this function, I **generate fake images**, **compute several types of losses** like **adversarial, cycle consistency, and identity loss**, because **combining these different losses helps the model learn better translations**. \n\nThen I **apply gradients** to **update my generator and discriminator models**. \n\nThis setup **makes sure each training step effectively improves my models**.\n","metadata":{}},{"cell_type":"code","source":"mse, mae = tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanAbsoluteError()\ng_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndM_opt, dP_opt = [tf.keras.optimizers.Adam(2e-4, beta_1=0.5) for _ in range(2)]\nLAMBDA_CYC, LAMBDA_ID = 10., 5.\n\n@tf.function\ndef train_step(real_x, real_y):\n    with tf.GradientTape(persistent=True) as tape:\n        fake_y = G(real_x, training=True)\n        fake_x = F(real_y, training=True)\n        cycled_x = F(fake_y, training=True)\n        cycled_y = G(fake_x, training=True)\n        same_y = G(real_y, training=True)\n        same_x = F(real_x, training=True)\n\n        # Explicitly specify training=True for discriminator calls\n        dM_real = DM(real_y, training=True)\n        dM_fake = DM(fake_y, training=True)\n        dP_real = DP(real_x, training=True)\n        dP_fake = DP(fake_x, training=True)\n\n        g_adv = mse(tf.ones_like(dM_fake), dM_fake)\n        f_adv = mse(tf.ones_like(dP_fake), dP_fake)\n        cycle = mae(real_x, cycled_x) + mae(real_y, cycled_y)\n        identity = mae(real_x, same_x) + mae(real_y, same_y)\n        g_loss = g_adv + f_adv + LAMBDA_CYC * cycle + LAMBDA_ID * identity\n        dM_loss = 0.5 * (mse(tf.ones_like(dM_real), dM_real) + mse(tf.zeros_like(dM_fake), dM_fake))\n        dP_loss = 0.5 * (mse(tf.ones_like(dP_real), dP_real) + mse(tf.zeros_like(dP_fake), dP_fake))\n\n    g_vars = G.trainable_variables + F.trainable_variables\n    g_opt.apply_gradients(zip(tape.gradient(g_loss, g_vars), g_vars))\n    dM_opt.apply_gradients(zip(tape.gradient(dM_loss, DM.trainable_variables), DM.trainable_variables))\n    dP_opt.apply_gradients(zip(tape.gradient(dP_loss, DP.trainable_variables), DP.trainable_variables))\n    return g_loss, dM_loss, dP_loss\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:59:15.497543Z","iopub.execute_input":"2025-05-25T12:59:15.497774Z","iopub.status.idle":"2025-05-25T12:59:15.515908Z","shell.execute_reply.started":"2025-05-25T12:59:15.497748Z","shell.execute_reply":"2025-05-25T12:59:15.515150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Training My CycleGAN  \n\nSince **I'm running this on Kaggle**, I **have access to a GPU**, but to **keep training time reasonable**, I'm **setting my training to run for only one epoch, with 800 steps.**\n\nThis **shorter training duration** isn't ideal for **perfect results**, but it's **good enough** to **demonstrate the functionality** and **initial performance of my CycleGAN**.\n\nThroughout the training, I **included progress print statements** to **monitor losses easily** and **quickly track how the training progresses step by step**.\n","metadata":{}},{"cell_type":"code","source":"EPOCHS, STEPS = 1, 800\nphoto_it, monet_it = iter(photo_ds), iter(monet_ds)\n\nfor ep in range(1, EPOCHS + 1):\n    print(f'\\nEpoch {ep}/{EPOCHS}')\n    for st in range(1, STEPS + 1):\n        gL, dML, dPL = train_step(next(photo_it), next(monet_it))\n        if st % 50 == 0 or st == 1 or st == STEPS:\n            print(f'Step {st}/{STEPS}: G_loss={gL:.3f}, DM_loss={dML:.3f}, DP_loss={dPL:.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:59:15.516792Z","iopub.execute_input":"2025-05-25T12:59:15.517370Z","iopub.status.idle":"2025-05-25T13:08:41.474880Z","shell.execute_reply.started":"2025-05-25T12:59:15.517351Z","shell.execute_reply":"2025-05-25T13:08:41.474244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Generating and Displaying Results\n\nAfter **training**, I **needed to see what my model learned**, so I **wrote a small piece of code** to **randomly select a few photos** and **convert them into Monet-style paintings**. \n\nBy **visually checking these results**, I can **quickly assess if my model effectively learned the Monet style**. \n\nThis **helps me understand visually** how **good or bad the initial performance is**.","metadata":{}},{"cell_type":"code","source":"def denorm(x):\n    return (x + 1) / 2\n\nsamps = random.sample(photo_paths, 3)\n\nplt.figure(figsize=(9, 6))\n\nfor i, p in enumerate(samps):\n    img = decode(p)\n    monet = G(tf.expand_dims(img, 0))[0]\n    \n    plt.subplot(3, 2, 2 * i + 1)\n    plt.imshow(denorm(img))\n    plt.axis('off')\n    plt.title('Photo')\n    \n    plt.subplot(3, 2, 2 * i + 2)\n    plt.imshow(denorm(monet))\n    plt.axis('off')\n    plt.title('Generated Monet')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:08:41.475703Z","iopub.execute_input":"2025-05-25T13:08:41.475981Z","iopub.status.idle":"2025-05-25T13:08:42.130232Z","shell.execute_reply.started":"2025-05-25T13:08:41.475954Z","shell.execute_reply":"2025-05-25T13:08:42.129474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Calculating FID Score\n\nFinally, I **calculated the Frechet Inception Distance (FID) score**, which **gives me an approximate measure** of how **realistic my generated Monet paintings are** compared to the **real Monet images**. \n\nI **only used 25 samples** for this calculation because I **wanted quick feedback** without **spending too much computing time**. \n\nThe **lower the FID score**, the **better my generated images are**. \n\nIf I had **more computational resources** and could **train for more epochs**, I **would expect the FID score to get lower**, meaning **better and more realistic images**.","metadata":{}},{"cell_type":"code","source":"# --- Quick FID (approx MiFID) on 25 samples (keeps runtime low) ---\nfrom scipy.linalg import sqrtm\nimport tensorflow.keras.applications.inception_v3 as iv3\nimport numpy as np  # Added import for NumPy\n\nincept = iv3.InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n\ndef feats(imgs):\n    imgs = tf.image.resize(imgs, (299,299))\n    imgs = iv3.preprocess_input(imgs*255.0)\n    return incept(imgs, training=False)\n\ndef calc_fid(real, fake):\n    # Convert tensors to NumPy arrays for the covariance computation.\n    mu1 = tf.nn.moments(real, 0)[0]\n    sigma1 = np.cov(real.numpy(), rowvar=False)\n    mu2 = tf.nn.moments(fake, 0)[0]\n    sigma2 = np.cov(fake.numpy(), rowvar=False)\n    ssdiff = tf.reduce_sum((mu1 - mu2)**2)\n    covmean = sqrtm(sigma1 @ sigma2)\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n    return float(ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean))\n\nN = 25\nreal_batch = tf.stack([decode(p) for p in monet_paths[:N]])\nfake_batch = tf.stack([G(tf.expand_dims(decode(p), 0), training=False)[0] for p in photo_paths[:N]])\n\nfid_value = calc_fid(feats(real_batch), feats(fake_batch))\nprint(f'Approximate FID on {N} samples: {fid_value:.2f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:08:42.131176Z","iopub.execute_input":"2025-05-25T13:08:42.131540Z","iopub.status.idle":"2025-05-25T13:08:53.665785Z","shell.execute_reply.started":"2025-05-25T13:08:42.131510Z","shell.execute_reply":"2025-05-25T13:08:53.665067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Submission part - generate & zip images for Kaggle\nimport os, shutil\nfrom PIL import Image\n\n\noutput_dir = \"images\"\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\nos.makedirs(output_dir)\n\n# Generate a Monet‐style image for each photo and save as JPG\nfor idx, p in enumerate(photo_paths, start=1):\n    # load and preprocess\n    img = decode(p)                              # [-1,1] float32 tensor, shape (256,256,3)\n    gen = G(tf.expand_dims(img, 0), training=False)[0]\n    # denormalize back to [0,255]\n    arr = ((gen * 127.5) + 127.5).numpy().astype(\"uint8\")\n    Image.fromarray(arr).save(f\"{output_dir}/{idx}.jpg\")\n\n# Zip the folder\nshutil.make_archive(\"images\", \"zip\", output_dir)\n\nprint(f\"Created submission archive: images.zip ({os.path.getsize('images.zip')/1e6:.1f} MB)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:11:12.829193Z","iopub.execute_input":"2025-05-25T13:11:12.829504Z","iopub.status.idle":"2025-05-25T13:19:05.455654Z","shell.execute_reply.started":"2025-05-25T13:11:12.829480Z","shell.execute_reply":"2025-05-25T13:19:05.454973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11. Summary and Future Improvements\n\nIn summary, my **CycleGAN model successfully demonstrated converting regular photos into Monet style paintings**, even with **limited training due to resource constraints**. \n\nWith **more epochs and better hardware**, I **believe the quality of the generated images would improve significantly**, as reflected in a **lower FID score**. \n\nOverall, this project **clearly shows that CycleGAN can effectively translate image styles** without **needing paired images**, which **makes it very useful for similar style-transfer applications**.","metadata":{}},{"cell_type":"markdown","source":"## References\n\n- **Kaggle “GANs Getting Started” Competition**  \n  https://www.kaggle.com/competitions/gan-getting-started  \n  (This is the official competition page where the Monet Painting Dataset is hosted.)\n\n- **CycleGAN Paper**  \n  Zhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). “Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.”  \n  https://arxiv.org/abs/1703.10593\n\n- **Instance Normalization**  \n  Ulyanov, D., Vedaldi, A., & Lempitsky, V. (2016). “Instance Normalization: The Missing Ingredient for Fast Stylization.”  \n  https://arxiv.org/abs/1607.08022\n\n- **TensorFlow Documentation**  \n  https://www.tensorflow.org/api_docs  \n  (I used this for building models, layers, and the Dataset API.)\n\n- **FID Metric Description**  \n  Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017). “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.”  \n  https://arxiv.org/abs/1706.08500\n\n- **InceptionV3 & FID Implementation**  \n  TensorFlow Keras Applications guide – https://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionV3  \n  SciPy’s `sqrtm` for covariance matrix square root – https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.sqrtm.html  \n","metadata":{}}]}